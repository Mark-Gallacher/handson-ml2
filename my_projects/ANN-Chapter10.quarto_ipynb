{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Chapter 10 - Introduction to ANN\n",
        "format: \n",
        "    html: \n",
        "        anchor-sections: true\n",
        "        smooth-scroll: true\n",
        "        code-line-numbers: true\n",
        "        code-links:\n",
        "            text: GitHub\n",
        "            icon: github\n",
        "            href: \"https://github.com/Mark-Gallacher/handson-ml2\"\n",
        "        link-external-icon: true\n",
        "        link-external-newwindow: true\n",
        "theme: \n",
        "    light: flatly\n",
        "    dark: darkly\n",
        "toc: true\n",
        "toc-depth: 2\n",
        "toc-title: \"Overview\"\n",
        "number-sections: true\n",
        "number-depth: 2\n",
        "---\n",
        "\n",
        "\n",
        "# Basic Perceptron\n",
        "\n",
        "## How does a Perceptron Model work?\n",
        "\n",
        "The inputs ($X$) are assigned weights ($W$) and passed to the core unit, which is usually a *threshold logic unit* (TLU) or \n",
        "a *linear threshold unit* (LTU). In the case of a TLU, a step function is applied to generate a binary output (1 or 0). \n",
        "This is usually a *Heaviside step function*, where every non-negative value is assigned the value 1, otherwise it is 0 (@eq-heaviside).\n",
        "\n",
        "\n",
        "The general formula to get the output of the Perceptron is given in @eq-perceptron. $\\phi$ represents the activation function, \n",
        "which in our case is the step function but there are some alternatives, such as the Hyperbolic Tangent Function (*tanh*), Rectified Linear\n",
        "Unit Function (ReLU). Finally, a bias term ($b$) is added on to every layer, apart the output layer, which usually is simply positive one.\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "heaviside(z) =  \n",
        "    \\begin{cases}\n",
        "         0 & \\text{if }z < 0, \\\\ \n",
        "         1 & \\text{if }z \\ge 0.\n",
        "    \\end{cases}\n",
        "\\end{equation}\n",
        "$$ {#eq-heaviside} \n",
        "\n",
        "$$ \n",
        "h_{W,b}(X) = \\phi(XW + b)\n",
        "$$ {#eq-perceptron}\n",
        "\n",
        "\n",
        "We can play around with the Perceptron thanks to `Perceptron` in **Scikit-Learn**.\n"
      ],
      "id": "3f1cceb6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data[:, (2, 3)]\n",
        "y = (iris.target == 0).astype(np.int32)"
      ],
      "id": "356db8f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "per_clf = Perceptron()\n",
        "per_clf.fit(x, y)\n",
        "\n",
        "y_pred = per_clf.predict([[2, 0.5]])\n",
        "print(y_pred)"
      ],
      "id": "99e18f4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Main Issue with Perceptrons\n",
        "\n",
        "There's nothing really new here if you have come across artificial neural networks, but this form is too simple to be useful. One issue is that is does not \n",
        "pass the XOR test, the exclusive-OR test. Notice how even when we pass the exact same data to the `fit()` and `predict()` methods, it is unable to correctly\n",
        "label the samples. This is essentially because the model is still linear, in the same sense logistic regression models are linear. Whereas the XOR problem \n",
        "requires a non-linear boundary to capture both the (0, 0) and (1, 1). \n"
      ],
      "id": "83c5743c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "per_clf = Perceptron()\n",
        "\n",
        "x = np.array([\n",
        "    0, 0,\n",
        "    1, 0,\n",
        "    0, 1,\n",
        "    1, 0\n",
        "]).reshape(4, 2)\n",
        "\n",
        "y = np.array([0, 1, 1, 0]).reshape(4, ) \n",
        "\n",
        "per_clf.fit(x, y)\n",
        "\n",
        "y_pred = per_clf.predict(x)\n",
        "print(y_pred)"
      ],
      "id": "7c761ab9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, how do solve this? One solution would be to use multiple layers of perceptrons, where the data is passed to an input layer, \n",
        "but their output is passed to a *hidden* layer. In this model, we have a fully connected network, where every unit in one layer is connected\n",
        "to every node in the next layer. These types of models are much more common and useful, as they go by the name of *Multilayer Perceptrons*.\n",
        "\n",
        "# Multilayer Perceptrons\n",
        "\n",
        "Multilayer Perceptrons (MLP) are made up of three layers, input, hidden and output.\n",
        "\n",
        "    - input: where the data enters the network\n",
        "    - hidden: one or more layers with TLU nodes\n",
        "    - output: last layer of TLU\n",
        "\n",
        "Given that a layer is influenced by the previous layer, and each layer has a set of tuning parameters,\n",
        "training a network becomes more challenging. The key breakthrough was the discovery or invention of the\n",
        "backpropagation, which employs gradient descent to find the optimal update that can minimise error.\n",
        "\n",
        "## Backpropagation\n",
        "\n",
        "The overview of the algorithm is:\n",
        "\n",
        "- It passes a set of data, a mini-batch, through the network until all samples are used up.\n",
        "    - Each pass of the complete training set is called an *Epoch*\n",
        "    - Meaning each epoch is made of multiple mini-batches.\n",
        "\n",
        "- For each mini-batch, a *forward pass* is completed, this is basically allowing the network to\n",
        "    generate an output for the supplied data. The key difference though is that each intermediate\n",
        "    result (output of each hidden layer) is stored for the next step.\n",
        "\n",
        "- Next, after we have the model predictions, we obtain the error using a loss function[^1].\n",
        "\n",
        "- Now, we work out how much each connection to the output added to the error. \n",
        "\n",
        "- Then we repeat this process, starting at the output layer until we reach the input layer\n",
        "    - Essentially, we want to know how much each connection contributes to the error in the next layer.\n",
        "\n",
        "- Once we have completed our backwards pass, we have the gradients we need to correct our whole network\n",
        "\n",
        "- So we simply compute the gradient descent uses these gradients.\n",
        "\n",
        "\n",
        "## Activation Functions\n",
        "\n",
        "If we remember the perceptrons typically used a step function as their activation function, we run into a problem.\n",
        "A step function is basically two horizontal lines, which both have a gradient of zero. We need a more curved shape,\n",
        "to give us and gradient descent useful gradients. When backpropagation was introduced, the substituted a sigmoid\n",
        "function (@eq-sigmoid) for the step function. However, the standard activation function is the ReLU function (@eq-relu) because of its simplicity\n",
        "both mathematically and computationally.\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + exp(z)}\n",
        "$$ {#eq-sigmoid}\n",
        "\n",
        "$$\n",
        "ReLU(z) = max(0, z)\n",
        "$$ {#eq-relu}\n",
        "\n",
        "In both these formulae, $z$ is passed to the functions but exactly is this value?\n",
        "\n",
        "The answer is pretty straight forward, it is the weighted sum of every connection, each with it own weight\n",
        "and a bias term. Since we are dealing with multiple layers, and each layer has multiple nodes, we \n",
        "describe this weighted sum for a given node, in a given layer as $z_{j}^{l}$, where $j$, and $l$ represent \n",
        "the node and the layer, respectively. One aspect which makes the formula a bit more complex, is that the \n",
        "weighted sum is influenced by the activation of the previous layer. In the full formula, @eq-activation, we \n",
        "need to sum across all the neuron in the previous layer ($l - 1$), $m$, and multiple the weight of a given connection\n",
        "by the activation of neuron it came from. In other words, if the previous layer has rendered the neuron quiet or inactive,\n",
        "this should be translated for our current layer, and so on.\n",
        "\n",
        "\n",
        "$$\n",
        "z_{j}^{l} = \\sum_{k = 1}^{m} w_{jk}^{l} a_{k}^{l - 1} + b_{j}^{l}\n",
        "$$ {#eq-activation}\n",
        "\n",
        "# Introduction to TensorFlow and Keras\n"
      ],
      "id": "2798a661"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.config.list_physical_devices()"
      ],
      "id": "640f01a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[^1]: This is usually the *Mean Squared Error*, but if the data has a lot of outliers, the *Mean Absolute Error* might be better. If you want \n",
        "to get fancy, you could even use *Huber Loss*, which is the combination of the too.\n"
      ],
      "id": "f18f6410"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\markg\\.virtualenvs\\hands-on-ml2-b1qHVUFy\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
